@inproceedings{Bogdanov:2013:EOL:2502081.2502229,
  author = {Bogdanov, Dmitry and Wack, Nicolas and G\'{o}mez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, Jos{\'e} and Serra, Xavier},
  title = {ESSENTIA: An Open-source Library for Sound and Music Analysis},
  booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
  series = {MM '13},
  year = {2013},
  isbn = {978-1-4503-2404-5},
  location = {Barcelona, Spain},
  pages = {855--858},
  numpages = {4},
  url = {http://doi.acm.org/10.1145/2502081.2502229},
  howpublished= {\url{http://essentia.upf.edu}}
  doi = {10.1145/2502081.2502229},
  acmid = {2502229},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {audio analysis, music information retrieval, open source, signal processing, sound and music computing},
}

@misc{BjarneStroustrupCite,
  title = {{Bjarne Stroustrup's FAQ}},
  author = {Bjarne Stroustrup},
  howpublished = {\url{http://www.stroustrup.com/bs_faq.html#really-say-that}},
  note = {Accessed: 2018-08-02},
}

@misc{683ea11abc74c43c6680cd4c08dc538caee546575b59c2f40d70033cf3389ec8,
  title = {{The Loudness War: Background, Speculation and Recommendations,{\textquotedblright} presented at the}},
  author = {Earl Vickers},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.301.8924},
  year = {2010},
  % note = {{There is growing concern that the quality of commercially distributed music is deteriorating as a result of mixing and mastering practices used in the so-called {\textquotedblleft}loudness war. {\textquotedblright} Due to the belief that {\textquotedblleft}louder is better, {\textquotedblright} dynamics compression is used to squeeze more and more loudness into the recordings. This paper reviews the history of the loudness war and explores some of its possible consequences, including aesthetic concerns and listening fatigue. Next, the loudness war is analyzed in terms of game theory. Evidence is presented to question the assumption that loudness is significantly correlated to listener preference and sales rankings. The paper concludes with practical recommendations for de-escalating the loudness war. 1.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@misc{EssentiaDissonance,
  title = {Essentia Dissonance Algorithm},
  howpublished = {\url{http://essentia.upf.edu/documentation/reference/std_Dissonance.html}},
  note = {Accessed: 2018-08-03}
}

@misc{89a5aac0af37ff45f55cd59468ed3b0a5f30cbb229bb691b7970477c14dbe1af,
title = {{Consonance and Dissonance}},
author = {Catherine Schmidt-Jones},
url = {http://cnx.org/content/m11953/1.13},
year = {2013},
note = {{Consonance and dissonance are musical terms describing whether combinations of notes sound good together or not.}},
keywords = {{tuning}},
keywords = {{music}},
keywords = {{interval}},
keywords = {{dissonant}},
keywords = {{dissonance}},
keywords = {{consonant}},
keywords = {{consonance}},
keywords = {{chords}},
language = {{en}},
}

@misc{e6fe2ea94b8d448139e05e3d36c0ffd5e82905dc87f719492ff3872650c667d9,
  title = {{Harmonic Pitch Class Profile (HPCP)}},
  author = {Eric Kang AND Abhipray Sahoo AND John Yan AND Chenxi Liu},
  url = {http://cnx.org/content/m48378/1.3},
  year = {2013},
  language = {{en}},
}

@misc{EssentiaChordDetection,
  title = {Essentia ChordDetection Algorithm},
  howpublished = {\url{http://essentia.upf.edu/documentation/reference/std_ChordsDetection.html}},
  note = {Accessed: 2018-08-03}
}

@techreport{ce55b6c29e8a29e37397d0218054117ec1cfa88bb8ff65b14f549c3e7eda7ceb,
  title = {{Dimensional Music Emotion Recognition: Combining Standard and Melodic Audio Features}},
  author = {Bruno Rocha AND Renato Panda AND Rui Pedro Paiva},
  publisher = {{Figshare}},
  url = {https://doi.org/10.6084/M9.FIGSHARE.1181786},
  year = {2014},
}

@misc{dadf933477b66ec1591840023fc37ac83b3e10d5aa4fd440639abca907d805ba,
  title = {{The music information retrieval evaluation exchange (2005-2007): A window into music information retrieval research}},
  author = {J. Stephen Downie},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.466.8371},
  year = {2008},
  % note = {{Abstract: The Music Information Retrieval Evaluation eXchange (MIREX) is the community-based framework for the formal evaluation of Music Information Retrieval (MIR) systems and algorithms. By looking at the background, structure, challenges, and contributions of MIREX this paper provides some insights into the world of MIR research. Because MIREX tasks are defined by the community they reflect the interests, techniques, and research paradigms of the community as a whole. Both MIREX and MIR have a strong bias toward audio-based approaches as most MIR researchers have strengths in signal processing. Spectral-based approaches to MIR tasks have led to advancements in the MIR field but they now appear to be reaching their limits of effectiveness. This limitation is called the {\textquoteleft}{\textquoteleft}glass ceiling{\textquoteright} {\textquoteright} problem and the MIREX results data support its existence. The post-hoc analyses of MIREX results data indicate that there are groups of systems that perform equally well within various MIR tasks. There are many challenges facing MIREX and MIR research most of which have their root causes in the intellectual property issues surrounding music. The current inability of researchers to test their approaches against the MIREX test collections outside the annual MIREX cycle is hindering the rapid development of improved MIR systems.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  keywords = {{Music information retrieval}},
  keywords = {{Evaluation}},
  keywords = {{MIREX PACS number}},
  keywords = {{43.75.Xz [doi}},
  keywords = {{10.1250/ast.29.247}},
  language = {{en}},
}

@book{lerch2012introduction,
  title={An Introduction to Audio Content Analysis: Applications in Signal Processing and Music Informatics},
  author={Lerch, A.},
  isbn={9781118393505},
  lccn={2012008107},
  % url={https://books.google.de/books?id=YSPT1LJqTbIC},
  year={2012},
  publisher={Wiley}
}

@techreport{43334da08db3748e0a566e71fbb76d92cf6f15f35575908aa975b0b2baddab5b,
title = {{Music Emotion Recognition: The Importance of Melodic Features}},
author = {Bruno Rocha},
publisher = {{Figshare}},
url = {https://doi.org/10.6084/M9.FIGSHARE.1181788},
year = {2014},
}

@misc{mirex_results_2017,
  title = {MIREX Results 2017},
  author = {Jongpil Lee, Jiyoung Park, Juhan Nam, Chanju Kim, Adrian Kim, Jangyeon Park, Jung-Woo Ha},
  howpublished = {\url{http://www.music-ir.org/mirex/abstracts/2017/LPNKK1.pdf}},
  note = {Accessed: 2018-08-04}
}

@misc{7cd5f337a4b030e3fafd0b4bc7e0976ff7cc1ec8c28d583c5dab695e0ee78941,
  title = {{A DEMONSTRATOR FOR AUTOMATIC MUSIC MOOD ESTIMATION}},
  author = {Janto Skowronek AND Martin Mckinney AND Steven Van De Par},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.205.9120},
  % note = {{Interest in automatic music mood classification is increasing because it could enable people to browse and manage their music collections by means of the music{\textquoteright}s emotional expression complementary to the widely used music genres. We continue our work on designing a well defined ground-truth database for music mood classification and show a demonstrator of automatic mood estimation. While a subjective evaluation of this algorithm on arbitrary music is ongoing, the initial classification results are encouraging and suggest that an automatic predicition of music mood is possible. 1}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@misc{8a02f9c512933d46fbea928d23ac65e38b61b88caba9b38319a5d4952b5a6667,
  title = {{The Role of Time in Music Emotion Recognition}},
  author = {Marcelo Caetano AND Frans Wiering},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.639.2116},
  % note = {{Abstract. Music is widely perceived as expressive of emotion. Music plays such a fundamental role in society economically, culturally and in people{\textquoteright}s personal lives that the emotional impact of music on people is extremely relevant. Research on automatic recognition of emotion in music usually approaches the problem from a classification perspective, comparing {\textquotedblleft}emotional labels {\textquotedblright} calculated from different representations of music with those of human annotators. Most music emotion recognition systems are just adapted genre classifiers, so the performance of music emotion recognition using this limited approach has held steady for the last few years because of several shortcomings. In this article, we discuss the importance of time, usually neglected in automatic recognition of emotion in music, and present ideas to exploit temporal information from the music and the listener{\textquoteright}s emotional ratings. We argue that only by incorporating time can we advance the present stagnant approach to music emotion recognition.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  keywords = {{Music}},
  keywords = {{Time}},
  keywords = {{Emotions}},
  keywords = {{Mood}},
  keywords = {{Automatic Mood Classifi- cation}},
  keywords = {{Music Emotion Recognition}},
  language = {{en}},
}

@article{russell1980circumplex,
  title={A circumplex model of affect.},
  author={Russell, James A},
  journal={Journal of personality and social psychology},
  volume={39},
  number={6},
  pages={1161},
  year={1980},
  publisher={American Psychological Association}
}

@misc{EssentiaFrameCutter,
  title = {Essentia FrameCutter Algorithm},
  howpublished = {\url{http://essentia.upf.edu/documentation/reference/std_FrameCutter.html}},
  note = {Accessed: 2018-08-07}
}

@ARTICLE{10.3389/fpsyg.2017.00440,
  author={Lee, Irene Eunyoung and Latchoumane, Charles-Francois V. and Jeong, Jaeseung},   
  title={Arousal Rules: An Empirical Investigation into the Aesthetic Experience of Cross-Modal Perception with Emotional Visual Music},      
  journal={Frontiers in Psychology},      
  volume={8},      
  pages={440},     
  year={2017},      
  url={https://www.frontiersin.org/article/10.3389/fpsyg.2017.00440},       
  doi={10.3389/fpsyg.2017.00440},      
  issn={1664-1078},   
  % abstract={Emotional visual music is a promising tool for the study of aesthetic perception in human psychology; however, the production of such stimuli and the mechanisms of auditory-visual emotion perception remain poorly understood. In Experiment 1, we suggested a literature-based, directive approach to emotional visual music design, and inspected the emotional meanings thereof using the self-rated psychometric and electroencephalographic (EEG) responses of the viewers. A two-dimensional (2D) approach to the assessment of emotion (the valence-arousal plane) with frontal alpha power asymmetry EEG (as a proposed index of valence) validated our visual music as an emotional stimulus. In Experiment 2, we used our synthetic stimuli to investigate possible underlying mechanisms of affective evaluation mechanisms in relation to audio and visual integration conditions between modalities (namely congruent, complementation, or incongruent combinations). In this experiment, we found that, when arousal information between auditory and visual modalities was contradictory (for example, active (+) on the audio channel but passive (-) on the video channel), the perceived emotion of cross-modal perception (visual music) followed the channel conveying the stronger arousal. Moreover, we found that an enhancement effect (heightened and compacted in subjects’ emotional responses) in the aesthetic perception of visual music might occur when the two channels contained contradictory arousal information and positive congruency in valence and texture/control. To the best of our knowledge, this work is the first to propose a literature-based directive production of emotional visual music prototypes and the validations thereof for the study of cross-modally evoked aesthetic experiences in human subjects.}
}

@misc{c0f471f7e6a618d880cf25175c9f99ac97ef8ba7d016c7f8c523f8d902892d9e,
  title = {{Color-emotion associations: Past experience and personal preference}},
  author = {Naz Kaya AND Helen H. Epps},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.616.6634},
  year = {2004},
  % note = {{This study examined color-emotion associations and the reasons for emotional reactions given to colors. Ten fully saturated chromatic colors were chosen from the Munsell color system: red, yellow, green, blue, purple, yellow-red, green-yellow, blue-green, purple-blue, and red-purple. Apart from these ten hue groups, three achromatic colors (white, black and gray) were also used. The sample consisted of 98 volunteered college students at a public institution in the southeast region of the US. Results revealed that the principle hues comprised the highest number of positive emotional responses, followed by the intermediate hues and the achromatic colors. Color symbolism seems to be apparent in how individuals associate colors with things, objects or physical space. Red-purple, for instance, was associated with the color of red wine, plum, bridesmaid dress, or the color of a bedroom. Overall, a color-related emotion was highly dependent on personal preference and one{\textquoteright}s past experience with that particular color.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@misc{905eee055abaf2a4f198ce11f35362a8963f61d552297a02dfc8fbc0c4f78679,
  title = {{BODY MOVEMENT IN MUSIC INFORMATION RETRIEVAL}},
  author = {Rolf Inge God{\o}y AND Alexander Refsum Jensenius},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.205.8981},
  % note = {{We can see many and strong links between music and human body movement in musical performance, in dance, and in the variety of movements that people make in listening situations. There is evidence that sensations of human body movement are integral to music as such, and that sensations of movement are efficient carriers of information about style, genre, expression, and emotions. The challenge now in MIR is to develop means for the extraction and representation of movement-inducing cues from musical sound, as well as to develop possibilities for using body movement as input to search and navigation interfaces in MIR. 1.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@misc{580abc6c6615ef9f9c16f9069351938a0dda3c5120b7e8d1450d6b1abf0a71df,
  title = {{Steering Behaviour for Computer based Intelligent Agents}},
  author = {Bhagya Shree Jain AND Shrikant Tiwari AND Sagar Chandrakar},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.682.2185},
  % note = {{This paper presents steering behaviours through intelligent agents populating virtual or simulated environments. We present a reliable effort towards automatic path planning of intelligent computer agents with the integration of trajectory optimisation and obstacle avoidance techniques. Our approach will increase the level of accuracy in movement and would be reliable in the sense that the best computable path will always be available. Behaviour programming is facilitated by a set of presentations of the environment that uses convenient frames of reference in two dimensional coordinate systems. Four different steering ways are examined for movement of intelligent agents. Combinations of steering behaviours can be used to achieve high level goals of movement or locomotion. Our approach can be tailored to suit any particular need which requires automation in locomotion.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@inproceedings{stam1999stable,
  title={Stable fluids},
  author={Stam, Jos},
  booktitle={Proceedings of the 26th annual conference on Computer graphics and interactive techniques},
  pages={121--128},
  year={1999},
  organization={ACM Press/Addison-Wesley Publishing Co.}
}

@misc{bcc7190da8e90284b4e790817b8eed4ee3ea6cffbe5a23ef07a000ca5628ffbc,
  title = {{Simplex Noise Demystified}},
  author = {Stefan Gustavson},
  publisher = {{Unpublished}},
  url = {https://doi.org/10.13140/RG.2.1.3369.6488},
  year = {2015},
  language = {{en}},
}

@misc{25a05da283ffd9d4bdda94c308ccf3a8759f22373b368f895cbef2e9186ab646,
  title = {{Gradientenbasierte Rauschfunktionen und Perlin Noise}},
  author = {Wilhelm Burger},
  publisher = {{Unpublished}},
  url = {https://doi.org/10.13140/RG.2.1.2641.2967},
  year = {2015},
  language = {{en}},
}

@book{nature_of_code,
  author = {Shiffman, Daniel},
  title = {The nature of code},
  publisher = {Self-publishing},
  year = {2012},
  isbn = {978-0-9859308-0-6},
}

@misc{LearnOpenGL,
  title = {Learn OpenGL},
  author = {Joey de Vries},
  howpublished = {\url{https://learnopengl.com/About}},
  note = {Accessed: 2018-08-13}
}

@article{ac8e278bab88e59aa3a147bef7b113350a723aa4547b89da74bdcadaf0244f1b,
  title = {{Xorshift RNGs}},
  author = {Marsaglia, George},
  url = {https://www.jstatsoft.org/index.php/jss/article/view/v008i14/xorshift.pdf},
  note = {{Description of a class of simple, extremely fast random number generators (RNGs) with periods 2k - 1 for k = 32, 64, 96, 128, 160,{\textquoteright}2. These RNGs seem to pass tests of randomness very well.}},
}

@misc{EssentiaPool,
  title = {Essentia Pool},
  howpublished = {\url{http://essentia.upf.edu/documentation/design_overview.html#pool}},
  note = {Accessed: 2018-08-18}
}

@article{smith1999bark,
  title={Bark and ERB bilinear transforms},
  author={Smith, Julius O and Abel, Jonathan S},
  journal={IEEE Transactions on speech and Audio Processing},
  volume={7},
  number={6},
  pages={697--708},
  year={1999},
  publisher={IEEE}
}

@article{AlajankiEmoInMusicAnalysis,
  author = {Alajanki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
  title = {Benchmarking music emotion recognition systems},
  journal = {PLOS ONE},
  year = {2016},
  note = {under review}
}

@book{rojas2013theorie,
  title={Theorie der neuronalen Netze: eine systematische Einf{\"u}hrung},
  author={Rojas, Raul},
  year={2013},
  publisher={Springer-Verlag}
}

@article{dalla2001developmental,
  title={A developmental study of the affective value of tempo and mode in music},
  author={Dalla Bella, Simone and Peretz, Isabelle and Rousseau, Luc and Gosselin, Nathalie},
  journal={Cognition},
  volume={80},
  number={3},
  pages={B1--B10},
  year={2001},
  publisher={Elsevier}
}

@misc{Intervalle,
  title = {WikiBooksIntervalle},
  howpublished = {\url{https://de.wikibooks.org/wiki/Musiklehre:_Intervalle}},
  note = {Accessed: 2018-08-23}
}

@misc{Klaviatur,
  title = {Klaviatur},
  howpublished = {\url{http://bertie.ch/musik/klaviatur/keyboard.gif}},
  note = {Accessed: 2018-08-23}
}

@book{MusiklehreTonleitern,
  title = {{Wikibooks: Musiklehre: Tonleitern}},
  url = {https://de.wikibooks.org/wiki/Musiklehre:_Tonleitern},
  % note = {{Musiklehre/ Navi Musiklehre {{ Musiklehre/ Navi Abschnitte Musiklehre/ Navi Harmonielehre }} Es existieren viele verschiedene Tonleitern. Die zwei wohl wichtigsten sind die Dur Tonleiter und Moll Tonleiter. Jede Tonleiter besteht aus einer bestimmten Anzahl von T{\"o}nen die in bestimmten Intervallen zueinander stehen. Dieses Intervallschema wiederholt sich im europ{\"a}ischen Musikraum in der Regel in jeder Oktave (12 Halbt{\"o}ne). =Dur Tonleiter= Jede Dur Tonleiter hat den gleichen Aufbau wie die Folge der Stammt{\"o}ne C D E F G A H C. Sie wird also gebildet mit den Tonschritten 1 1 {\textonehalf} 1 1 1 {\textonehalf} Zum Beispiel wird die D Dur Tonleiter gebildet durch die T{\"o}ne D E Fis G A H Cis D . Es ist {\"u}blich eine Dur Tonleiter mit gro{\ss}geschriebenem Grundton zu notieren. =Moll Tonleiter= =Nat{\"u}rliche Moll Tonleiter= Die Grundform einer Moll Tonleiter ist aufgebaut wie die Folge der Stammt{\"o}ne A H C D E F G A. Sie wird also gebildet mit den Tonschritten 1 {\textonehalf} 1 1 {\textonehalf} 1 1 Diese Grundform wird auch die nat{\"u}rliche oder reine Moll Tonleiter genannt. =Harmonische Moll Tonleiter= Weil der letzte Schritt in der nat{\"u}rlichen Moll Tonleiter ein ganzer Tonschritt ist fehlt in dieser Grundform die Wirkung eines Leittons. Um diese Wirkung zu erzielen gibt es eine Variante der nat{\"u}rlichen Moll Tonleiter die harmonische Moll Tonleiter bei der die siebte Stufe um einen Halbton erh{\"o}ht ist. Sie ist aufgebaut mit den Tonschritten 1 {\textonehalf} 1 1 {\textonehalf} 1{\textonehalf} {\textonehalf} Die harmonische a Moll Tonleiter ist also A H C D E F Gis A. Im Unterschied zu den Dur Tonleitern werden die Moll Tonleitern {\"u}blicherweise mit kleingeschriebenem Grundton notiert . =Melodische Moll Tonleiter= Die erh{\"o}hte Stufe in der harmonischen Moll Tonleiter f{\"u}hrt zu einem Tonschritt zwischen der sechsten und der siebten Stufe von anderthalb Ganzt{\"o}nen. Um dies zu vermeiden wird beim melodischen Moll die sechste Stufe ebenfalls erh{\"o}ht. Damit entspricht die Skala bis auf die kleine Terz der Dur Skala. Da der k{\"u}nstlich erzeugte Leitton beim Abw{\"a}rtsgehen nicht n{\"o}tig ist wird dann das nat{\"u}rliche Moll verwendet. Die melodische Moll Tonleiter ist also aufw{\"a}rts aufgebaut mit den Tonschritten 1 {\textonehalf} 1 1 1 1 {\textonehalf} und abw{\"a}rts wie die nat{\"u}rliche Moll Tonleiter 1 {\textonehalf} 1 1 {\textonehalf} 1 1 Die melodische a Moll Tonleiter lautet aufw{\"a}rts A H C D E Fis Gis A und abw{\"a}rts A G F E D C H A. Dur/Moll Tonleitern bestehen in ihrer Grundform immer aus 7 verschiedenen T{\"o}nen mit folgender Intervallstruktur Dur Tonleiter 1 1 {\textonehalf} 1 1 1 {\textonehalf} (= 5 Ganztonschritte + 2 Halbtonschritte = 6 Ganztonschritte = 12 Halbtonschritte = 1 Oktave) Moll Tonleiter 1 {\textonehalf} 1 1 {\textonehalf} 1 1 (= 5 Ganztonschritte + 2 Halbtonschritte = 6 Ganztonschritte = 12 Halbtonschritte = 1 Oktave) Dur und Moll Leitern wiederholen sich also in jeder Oktave. Das Schema soll im Folgenden anhand der C Dur Tonleiter verdeutlicht werden. =Die Dur Tonleiter= =C Dur= Zun{\"a}chst nehmen wir alle T{\"o}ne im chromatischen Tonraum (mit Intervallen zwischen den T{\"o}nen). {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} (= 12 Halbtonschritte = 1 Oktave) C C$\#$ D D$\#$ E F F$\#$ G G$\#$ A A$\#$ B (C) Nun werden ausgehend vom Grundton (bei der C Dur Tonleiter nat{\"u}rlich das C) die Intervalle des Dur Schemas abgez{\"a}hlt {\textonehalf} + {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} = 1 1 {\textonehalf} 1 1 1 {\textonehalf} / \ / \ / \ / \ / \ / \ / \ C C$\#$ D D$\#$ E F F$\#$ G G$\#$ A A$\#$ B (C) C D E F G A B (C) So ergibt sich die C Dur Tonleiter. Analog funktioniert dies auch f{\"u}r jede andere Tonleiter nat{\"u}rlich dann mit anderer Intervallstruktur und dem entsprechenden Grundton. Die C Dur Tonleiter gilt als die wichtigste Tonleiter. Ihre Bedeutung verdankt sie vor allem der Tastatur Einteilung auf dem Klavier. Wenn man die Klaviatur einmal n{\"a}her betrachtet so kommt man zu dem Ergebnis dass alle wei{\ss}en Tasten die C Dur Tonleiter bilden. Auff{\"a}llig ist dass die C Dur Tonleiter nur aus T{\"o}nen ohne Vorzeichen also Stammt{\"o}nen besteht weshalb man denken k{\"o}nnte dass diese Tonleiter keine Halbtonschritte kennt. Dies ist jedoch ein fataler Irrtum den vor allem Anf{\"a}nger begehen. Die C Dur Tonleiter enth{\"a}lt n{\"a}mlich sehr wohl Halbtonschritte obgleich sie in der Notenschrift versteckt sind. Zwischen den T{\"o}nen E und F befindet sich ein Halbtonschritt genauso wie zwischen H und C . Obwohl dies nicht aus der Notenschrift ersichtlich ist hat gerade diese Eigenschaft entscheidende Auswirkungen auf die theoretische Sicht aller Tonleitersysteme. =G Dur= Versuchen wir nun analog die G Dur Tonleiter herzuleiten. Zun{\"a}chst nehmen wir wieder alle T{\"o}ne im chromatischen Tonraum (der {\"U}bersicht halber diesmal von G angefangen) . {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} {\textonehalf} (= 12 Halbtonschritte = 1 Oktave) G G$\#$ A A$\#$ B C C$\#$ D D$\#$ E F F$\#$ (G) Nun werden ausgehend vom Grundton bei G Dur somit das G wieder die Dur Intervalle abgez{\"a}hlt {\textonehalf} + {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} = 1 1 {\textonehalf} 1 1 1 {\textonehalf} / \ / \ / \ / \ / \ / \ / \ G G$\#$ A A$\#$ B C C$\#$ D D$\#$ E F F$\#$ (G) G A B C D E F$\#$ (G) G Dur besteht also aus den T{\"o}nen G A B C D E F$\#$. Es ist also ein Ton mit einem Erh{\"o}hungzeichen n{\"a}mlich das F$\#$ enthalten. Aus diesem Grund wird G Dur auch mit einem $\#$ nach dem Notenschl{\"u}ssel notiert. Das $\#$ schlie{\ss}t dann die f{\"u}nfte Notenzeile ein. Das ist die Zeile auf der normalerweise die Note F steht. Das Erh{\"o}hungszeichen $\#$ macht aus jeder Note die auf der f{\"u}nften Zeile steht ein F$\#$. =Die Moll Tonleiter= =a Moll= Analog zu den Dur Tonleitern werden ausgehend vom Grundton die Intervalle des Moll Schemas abgez{\"a}hlt {\textonehalf} + {\textonehalf} {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} {\textonehalf} + {\textonehalf} {\textonehalf} + {\textonehalf} = 1 {\textonehalf} 1 1 {\textonehalf} 1 1 / \ / \ / \ / \ / \ / \ / \ A A$\#$ B C C$\#$ D D$\#$ E F F$\#$ G G$\#$ (A) A B C D E F G (A) Sie besteht aus denselben T{\"o}nen wie C Dur wird daher parallele Tonart zu C Dur genannt. Die beiden Tonleitern unterscheiden sich lediglich durch ihren Grundton. Vorlage Navigation hoch}},
  language = {{ger}},
}

@misc{KosTrans,
  title = {Technik der Fourier-Transformation},
  howpublished = {\url{https://www.uni-muenster.de/imperia/md/content/physikalische_chemie/praktikum/fourier__transformation__kr_mer__elisabeth__schmitz__rene_.pdf}},
  note = {Accessed: 2018-08-23}
}

@book{kammeyer2013digitale,
  title={Digitale Signalverarbeitung: Filterung und Spektralanalyse mit MATLAB-Übungen},
  author={Kammeyer, Karl-Dirk and Kroschel, Kristian},
  year={2013},
  publisher={Springer-Verlag},
  isbn = {978-3-658-20134-0}
}

@misc{BBands,
  title = {Sound Perception, The auditory filter},
  howpublished = {\url{http://home.ieis.tue.nl/dhermes/lectures/soundperception/04AuditoryFilter.html}},
  note = {Accessed: 2018-08-23}
}

@article{hawley2004benefit,
  title={The benefit of binaural hearing in a cocktail party: Effect of location and type of interferer},
  author={Hawley, Monica L and Litovsky, Ruth Y and Culling, John F},
  journal={The Journal of the Acoustical Society of America},
  volume={115},
  number={2},
  pages={833--843},
  year={2004},
  publisher={ASA}
}

@misc{adobeAfterEffects,
  title = {Adope After Effects},
  howpublished = {\url{https://www.adobe.com/de/products/aftereffects.html#x}},
  note = {Accessed: 2018-08-23}
}

@misc{appleITunes,
  title = {Apple iTunes},
  howpublished = {\url{https://www.apple.com/de/itunes/}},
  note = {Accessed: 2018-08-23}
}

@misc{MusikAnimationMachine,
  title = {Music Animation Machine},
  author = {Stephen Malinowski},
  howpublished = {\url{https://www.musanim.com/watch_mam.html}},
  note = {Accessed: 2018-08-23}
}

@misc{clamProjekt,
  title = {CLAM: C++ Library for Audio and Music},
  howpublished = {\url{http://clam-project.org/}},
  note = {Accessed: 2018-08-23}
}

@misc{clamProjektNetEditor,
  title = {CLAM - Network Editor},
  howpublished = {\url{http://clam-project.org/wiki/Network_Editor_tutorial}},
  note = {Accessed: 2018-08-23}
}

@misc{clamProjektChorData,
  title = {CLAM - Chordata},
  howpublished = {\url{http://clam-project.org/wiki/Chordata_tutorial}},
  note = {Accessed: 2018-08-23}
}


@misc{sphereGeneration,
  title = {Stack Overflow - Sphere Generation},
  author = {Kevin},
  howpublished = {\url{https://stackoverflow.com/questions/7687148/drawing-sphere-in-opengl-without-using-glusphere}},
  note = {Accessed: 2018-08-24}
}

@misc{pandasDoc,
  title = {Pandas},
  howpublished = {\url{https://pandas.pydata.org/}},
  note = {Accessed: 2018-08-25}
}

@misc{kerasDoc,
  title = {Keras Documentation},
  howpublished = {\url{https://keras.io/}},
  note = {Accessed: 2018-08-25}
}
