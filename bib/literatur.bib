@inproceedings{Bogdanov:2013:EOL:2502081.2502229,
  author = {Bogdanov, Dmitry and Wack, Nicolas and G\'{o}mez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, Jos{\'e} and Serra, Xavier},
  title = {ESSENTIA: An Open-source Library for Sound and Music Analysis},
  booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
  series = {MM '13},
  year = {2013},
  isbn = {978-1-4503-2404-5},
  location = {Barcelona, Spain},
  pages = {855--858},
  numpages = {4},
  url = {http://doi.acm.org/10.1145/2502081.2502229},
  howpublished= {\url{http://essentia.upf.edu}}
  doi = {10.1145/2502081.2502229},
  acmid = {2502229},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {audio analysis, music information retrieval, open source, signal processing, sound and music computing},
}

@misc{BjarneStroustrupCite,
  title = {{Bjarne Stroustrup's FAQ}},
  author = {Bjarne Stroustrup},
  howpublished = {\url{http://www.stroustrup.com/bs_faq.html#really-say-that}},
  note = {Accessed: 2018-08-02},
}

@misc{683ea11abc74c43c6680cd4c08dc538caee546575b59c2f40d70033cf3389ec8,
  title = {{The Loudness War: Background, Speculation and Recommendations,{\textquotedblright} presented at the}},
  author = {Earl Vickers},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.301.8924},
  year = {2010},
  % note = {{There is growing concern that the quality of commercially distributed music is deteriorating as a result of mixing and mastering practices used in the so-called {\textquotedblleft}loudness war. {\textquotedblright} Due to the belief that {\textquotedblleft}louder is better, {\textquotedblright} dynamics compression is used to squeeze more and more loudness into the recordings. This paper reviews the history of the loudness war and explores some of its possible consequences, including aesthetic concerns and listening fatigue. Next, the loudness war is analyzed in terms of game theory. Evidence is presented to question the assumption that loudness is significantly correlated to listener preference and sales rankings. The paper concludes with practical recommendations for de-escalating the loudness war. 1.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@misc{EssentiaDissonance,
  title = {Essentia Dissonance Algorithm},
  howpublished = {\url{http://essentia.upf.edu/documentation/reference/std_Dissonance.html}},
  note = {Accessed: 2018-08-03}
}

@misc{89a5aac0af37ff45f55cd59468ed3b0a5f30cbb229bb691b7970477c14dbe1af,
title = {{Consonance and Dissonance}},
author = {Catherine Schmidt-Jones},
url = {http://cnx.org/content/m11953/1.13},
year = {2013},
note = {{Consonance and dissonance are musical terms describing whether combinations of notes sound good together or not.}},
keywords = {{tuning}},
keywords = {{music}},
keywords = {{interval}},
keywords = {{dissonant}},
keywords = {{dissonance}},
keywords = {{consonant}},
keywords = {{consonance}},
keywords = {{chords}},
language = {{en}},
}

@misc{e6fe2ea94b8d448139e05e3d36c0ffd5e82905dc87f719492ff3872650c667d9,
  title = {{Harmonic Pitch Class Profile (HPCP)}},
  author = {Eric Kang AND Abhipray Sahoo AND John Yan AND Chenxi Liu},
  url = {http://cnx.org/content/m48378/1.3},
  year = {2013},
  language = {{en}},
}

@misc{EssentiaChordDetection,
  title = {Essentia ChordDetection Algorithm},
  howpublished = {\url{http://essentia.upf.edu/documentation/reference/std_ChordsDetection.html}},
  note = {Accessed: 2018-08-03}
}

@techreport{ce55b6c29e8a29e37397d0218054117ec1cfa88bb8ff65b14f549c3e7eda7ceb,
  title = {{Dimensional Music Emotion Recognition: Combining Standard and Melodic Audio Features}},
  author = {Bruno Rocha AND Renato Panda AND Rui Pedro Paiva},
  publisher = {{Figshare}},
  url = {https://doi.org/10.6084/M9.FIGSHARE.1181786},
  year = {2014},
}

@misc{dadf933477b66ec1591840023fc37ac83b3e10d5aa4fd440639abca907d805ba,
  title = {{The music information retrieval evaluation exchange (2005-2007): A window into music information retrieval research}},
  author = {J. Stephen Downie},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.466.8371},
  year = {2008},
  % note = {{Abstract: The Music Information Retrieval Evaluation eXchange (MIREX) is the community-based framework for the formal evaluation of Music Information Retrieval (MIR) systems and algorithms. By looking at the background, structure, challenges, and contributions of MIREX this paper provides some insights into the world of MIR research. Because MIREX tasks are defined by the community they reflect the interests, techniques, and research paradigms of the community as a whole. Both MIREX and MIR have a strong bias toward audio-based approaches as most MIR researchers have strengths in signal processing. Spectral-based approaches to MIR tasks have led to advancements in the MIR field but they now appear to be reaching their limits of effectiveness. This limitation is called the {\textquoteleft}{\textquoteleft}glass ceiling{\textquoteright} {\textquoteright} problem and the MIREX results data support its existence. The post-hoc analyses of MIREX results data indicate that there are groups of systems that perform equally well within various MIR tasks. There are many challenges facing MIREX and MIR research most of which have their root causes in the intellectual property issues surrounding music. The current inability of researchers to test their approaches against the MIREX test collections outside the annual MIREX cycle is hindering the rapid development of improved MIR systems.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  keywords = {{Music information retrieval}},
  keywords = {{Evaluation}},
  keywords = {{MIREX PACS number}},
  keywords = {{43.75.Xz [doi}},
  keywords = {{10.1250/ast.29.247}},
  language = {{en}},
}

@book{lerch2012introduction,
  title={An Introduction to Audio Content Analysis: Applications in Signal Processing and Music Informatics},
  author={Lerch, A.},
  isbn={9781118393505},
  lccn={2012008107},
  % url={https://books.google.de/books?id=YSPT1LJqTbIC},
  year={2012},
  publisher={Wiley}
}

@techreport{43334da08db3748e0a566e71fbb76d92cf6f15f35575908aa975b0b2baddab5b,
title = {{Music Emotion Recognition: The Importance of Melodic Features}},
author = {Bruno Rocha},
publisher = {{Figshare}},
url = {https://doi.org/10.6084/M9.FIGSHARE.1181788},
year = {2014},
}

@misc{mirex_results_2017,
  title = {MIREX Results 2017},
  author = {Jongpil Lee, Jiyoung Park, Juhan Nam, Chanju Kim, Adrian Kim, Jangyeon Park, Jung-Woo Ha},
  howpublished = {\url{http://www.music-ir.org/mirex/abstracts/2017/LPNKK1.pdf}},
  note = {Accessed: 2018-08-04}
}


@misc{7cd5f337a4b030e3fafd0b4bc7e0976ff7cc1ec8c28d583c5dab695e0ee78941,
  title = {{A DEMONSTRATOR FOR AUTOMATIC MUSIC MOOD ESTIMATION}},
  author = {Janto Skowronek AND Martin Mckinney AND Steven Van De Par},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.205.9120},
  % note = {{Interest in automatic music mood classification is increasing because it could enable people to browse and manage their music collections by means of the music{\textquoteright}s emotional expression complementary to the widely used music genres. We continue our work on designing a well defined ground-truth database for music mood classification and show a demonstrator of automatic mood estimation. While a subjective evaluation of this algorithm on arbitrary music is ongoing, the initial classification results are encouraging and suggest that an automatic predicition of music mood is possible. 1}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@misc{8a02f9c512933d46fbea928d23ac65e38b61b88caba9b38319a5d4952b5a6667,
  title = {{The Role of Time in Music Emotion Recognition}},
  author = {Marcelo Caetano AND Frans Wiering},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.639.2116},
  % note = {{Abstract. Music is widely perceived as expressive of emotion. Music plays such a fundamental role in society economically, culturally and in people{\textquoteright}s personal lives that the emotional impact of music on people is extremely relevant. Research on automatic recognition of emotion in music usually approaches the problem from a classification perspective, comparing {\textquotedblleft}emotional labels {\textquotedblright} calculated from different representations of music with those of human annotators. Most music emotion recognition systems are just adapted genre classifiers, so the performance of music emotion recognition using this limited approach has held steady for the last few years because of several shortcomings. In this article, we discuss the importance of time, usually neglected in automatic recognition of emotion in music, and present ideas to exploit temporal information from the music and the listener{\textquoteright}s emotional ratings. We argue that only by incorporating time can we advance the present stagnant approach to music emotion recognition.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  keywords = {{Music}},
  keywords = {{Time}},
  keywords = {{Emotions}},
  keywords = {{Mood}},
  keywords = {{Automatic Mood Classifi- cation}},
  keywords = {{Music Emotion Recognition}},
  language = {{en}},
}

@article{russell1980circumplex,
  title={A circumplex model of affect.},
  author={Russell, James A},
  journal={Journal of personality and social psychology},
  volume={39},
  number={6},
  pages={1161},
  year={1980},
  publisher={American Psychological Association}
}

@misc{EssentiaFrameCutter,
  title = {Essentia FrameCutter Algorithm},
  howpublished = {\url{http://essentia.upf.edu/documentation/reference/std_FrameCutter.html}},
  note = {Accessed: 2018-08-07}
}

@ARTICLE{10.3389/fpsyg.2017.00440,
  author={Lee, Irene Eunyoung and Latchoumane, Charles-Francois V. and Jeong, Jaeseung},   
  title={Arousal Rules: An Empirical Investigation into the Aesthetic Experience of Cross-Modal Perception with Emotional Visual Music},      
  journal={Frontiers in Psychology},      
  volume={8},      
  pages={440},     
  year={2017},      
  url={https://www.frontiersin.org/article/10.3389/fpsyg.2017.00440},       
  doi={10.3389/fpsyg.2017.00440},      
  issn={1664-1078},   
  % abstract={Emotional visual music is a promising tool for the study of aesthetic perception in human psychology; however, the production of such stimuli and the mechanisms of auditory-visual emotion perception remain poorly understood. In Experiment 1, we suggested a literature-based, directive approach to emotional visual music design, and inspected the emotional meanings thereof using the self-rated psychometric and electroencephalographic (EEG) responses of the viewers. A two-dimensional (2D) approach to the assessment of emotion (the valence-arousal plane) with frontal alpha power asymmetry EEG (as a proposed index of valence) validated our visual music as an emotional stimulus. In Experiment 2, we used our synthetic stimuli to investigate possible underlying mechanisms of affective evaluation mechanisms in relation to audio and visual integration conditions between modalities (namely congruent, complementation, or incongruent combinations). In this experiment, we found that, when arousal information between auditory and visual modalities was contradictory (for example, active (+) on the audio channel but passive (-) on the video channel), the perceived emotion of cross-modal perception (visual music) followed the channel conveying the stronger arousal. Moreover, we found that an enhancement effect (heightened and compacted in subjects’ emotional responses) in the aesthetic perception of visual music might occur when the two channels contained contradictory arousal information and positive congruency in valence and texture/control. To the best of our knowledge, this work is the first to propose a literature-based directive production of emotional visual music prototypes and the validations thereof for the study of cross-modally evoked aesthetic experiences in human subjects.}
}

@misc{c0f471f7e6a618d880cf25175c9f99ac97ef8ba7d016c7f8c523f8d902892d9e,
  title = {{Color-emotion associations: Past experience and personal preference}},
  author = {Naz Kaya AND Helen H. Epps},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.616.6634},
  year = {2004},
  % note = {{This study examined color-emotion associations and the reasons for emotional reactions given to colors. Ten fully saturated chromatic colors were chosen from the Munsell color system: red, yellow, green, blue, purple, yellow-red, green-yellow, blue-green, purple-blue, and red-purple. Apart from these ten hue groups, three achromatic colors (white, black and gray) were also used. The sample consisted of 98 volunteered college students at a public institution in the southeast region of the US. Results revealed that the principle hues comprised the highest number of positive emotional responses, followed by the intermediate hues and the achromatic colors. Color symbolism seems to be apparent in how individuals associate colors with things, objects or physical space. Red-purple, for instance, was associated with the color of red wine, plum, bridesmaid dress, or the color of a bedroom. Overall, a color-related emotion was highly dependent on personal preference and one{\textquoteright}s past experience with that particular color.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@misc{905eee055abaf2a4f198ce11f35362a8963f61d552297a02dfc8fbc0c4f78679,
  title = {{BODY MOVEMENT IN MUSIC INFORMATION RETRIEVAL}},
  author = {Rolf Inge God{\o}y AND Alexander Refsum Jensenius},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.205.8981},
  % note = {{We can see many and strong links between music and human body movement in musical performance, in dance, and in the variety of movements that people make in listening situations. There is evidence that sensations of human body movement are integral to music as such, and that sensations of movement are efficient carriers of information about style, genre, expression, and emotions. The challenge now in MIR is to develop means for the extraction and representation of movement-inducing cues from musical sound, as well as to develop possibilities for using body movement as input to search and navigation interfaces in MIR. 1.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@misc{580abc6c6615ef9f9c16f9069351938a0dda3c5120b7e8d1450d6b1abf0a71df,
  title = {{Steering Behaviour for Computer based Intelligent Agents}},
  author = {Bhagya Shree Jain AND Shrikant Tiwari AND Sagar Chandrakar},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.682.2185},
  % note = {{This paper presents steering behaviours through intelligent agents populating virtual or simulated environments. We present a reliable effort towards automatic path planning of intelligent computer agents with the integration of trajectory optimisation and obstacle avoidance techniques. Our approach will increase the level of accuracy in movement and would be reliable in the sense that the best computable path will always be available. Behaviour programming is facilitated by a set of presentations of the environment that uses convenient frames of reference in two dimensional coordinate systems. Four different steering ways are examined for movement of intelligent agents. Combinations of steering behaviours can be used to achieve high level goals of movement or locomotion. Our approach can be tailored to suit any particular need which requires automation in locomotion.}},
  contributor = {{The Pennsylvania State University CiteSeerX Archives}},
  language = {{en}},
}

@inproceedings{stam1999stable,
  title={Stable fluids},
  author={Stam, Jos},
  booktitle={Proceedings of the 26th annual conference on Computer graphics and interactive techniques},
  pages={121--128},
  year={1999},
  organization={ACM Press/Addison-Wesley Publishing Co.}
}

@misc{bcc7190da8e90284b4e790817b8eed4ee3ea6cffbe5a23ef07a000ca5628ffbc,
  title = {{Simplex Noise Demystified}},
  author = {Stefan Gustavson},
  publisher = {{Unpublished}},
  url = {https://doi.org/10.13140/RG.2.1.3369.6488},
  year = {2015},
  language = {{en}},
}

@misc{25a05da283ffd9d4bdda94c308ccf3a8759f22373b368f895cbef2e9186ab646,
  title = {{Gradientenbasierte Rauschfunktionen und Perlin Noise}},
  author = {Wilhelm Burger},
  publisher = {{Unpublished}},
  url = {https://doi.org/10.13140/RG.2.1.2641.2967},
  year = {2015},
  language = {{en}},
}

@book{nature_of_code,
  author = {Shiffman, Daniel},
  title = {The nature of code},
  publisher = {Self-publishing},
  year = {2012},
  isbn = {978-0-9859308-0-6},
}
